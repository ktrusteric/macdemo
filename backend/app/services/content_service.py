from typing import List, Optional, Dict, Any
from pymongo.database import Database
from bson import ObjectId
from app.models.content import Content, ContentType
from datetime import datetime
import logging

logger = logging.getLogger(__name__)

class ContentService:
    def __init__(self, database: Database):
        self.db = database
        self.collection = self.db.content

    async def create_content(self, content: Content) -> Content:
        """åˆ›å»ºå†…å®¹"""
        try:
            content_dict = content.dict()
            content_dict['_id'] = ObjectId()
            
            result = await self.collection.insert_one(content_dict)
            content.id = str(result.inserted_id)
            
            return content
        except Exception as e:
            raise Exception(f"Failed to create content: {str(e)}")

    def _map_document_to_content(self, document: dict) -> Content:
        """å°†æ•°æ®åº“æ–‡æ¡£æ˜ å°„ä¸ºContentå¯¹è±¡ï¼ˆæ”¯æŒä¸­è‹±æ–‡å­—æ®µï¼‰"""
        try:
            # æ™ºèƒ½å­—æ®µæ˜ å°„ï¼šä¼˜å…ˆä½¿ç”¨è‹±æ–‡å­—æ®µï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨ä¸­æ–‡å­—æ®µ
            mapped_doc = {
                "id": str(document["_id"]),
                
                # æ ‡é¢˜å­—æ®µ - ä¼˜å…ˆè‹±æ–‡ï¼Œåå¤‡ä¸­æ–‡
                "title": document.get("title") or document.get("æ ‡é¢˜", "æ— æ ‡é¢˜"),
                
                # å†…å®¹å­—æ®µ - ä¼˜å…ˆè‹±æ–‡ï¼Œåå¤‡ä¸­æ–‡
                "content": document.get("content") or document.get("æ–‡ç« å†…å®¹", "æ— å†…å®¹"),
                
                # æ¥æºå­—æ®µ - ä¼˜å…ˆè‹±æ–‡ï¼Œåå¤‡ä¸­æ–‡
                "source": document.get("source") or document.get("æ¥æºæœºæ„", "æœªçŸ¥æ¥æº"),
                
                # é“¾æ¥å­—æ®µ - ä¼˜å…ˆè‹±æ–‡ï¼Œåå¤‡ä¸­æ–‡
                "link": document.get("link") or document.get("é“¾æ¥", ""),
                
                # å‘å¸ƒæ—¶é—´ - ä¼˜å…ˆè‹±æ–‡ï¼Œåå¤‡ä¸­æ–‡ï¼Œæœ€åé»˜è®¤å½“å‰æ—¶é—´
                "publish_time": self._parse_publish_time(document),
                
                # å‘å¸ƒæ—¥æœŸ - å­—ç¬¦ä¸²æ ¼å¼YYYY-MM-DDï¼Œç”¨äºå‰ç«¯æ’åº
                "publish_date": document.get("publish_date") or (
                    self._parse_publish_time(document).strftime("%Y-%m-%d") if self._parse_publish_time(document) else None
                ),
                
                # å¤„ç†æ–‡æ¡£ç±»å‹æ˜ å°„ - ä¼˜å…ˆè‹±æ–‡ï¼Œåå¤‡ä¸­æ–‡
                "type": document.get("type") or self._map_document_type(document.get("æ–‡æ¡£ç±»å‹", "è¡Œä¸šèµ„è®¯")),
                
                # æ ‡ç­¾å­—æ®µï¼ˆéƒ½æ˜¯è‹±æ–‡å­—æ®µåï¼‰
                "basic_info_tags": self._ensure_list(document.get("basic_info_tags", [])),
                "region_tags": self._ensure_list(document.get("region_tags", [])),
                "energy_type_tags": self._ensure_list(document.get("energy_type_tags", [])),
                "business_field_tags": self._ensure_list(document.get("business_field_tags", [])),
                "beneficiary_tags": self._ensure_list(document.get("beneficiary_tags", [])),
                "policy_measure_tags": self._ensure_list(document.get("policy_measure_tags", [])),
                "importance_tags": self._ensure_list(document.get("importance_tags", [])),
                
                # æ—¶é—´å­—æ®µ - ä¼˜å…ˆè‹±æ–‡ï¼Œåå¤‡ä¸­æ–‡
                "created_at": self._parse_datetime(document.get("created_at") or document.get("å¯¼å…¥æ—¶é—´")),
                "updated_at": self._parse_datetime(document.get("updated_at") or document.get("å¯¼å…¥æ—¶é—´")),
                "view_count": max(0, int(document.get("view_count", 0)))  # ç¡®ä¿éè´Ÿæ•°
            }
            
            return Content(**mapped_doc)
            
        except Exception as e:
            error_msg = f"Failed to map document to content: {str(e)}"
            logger.error(f"{error_msg} - Document ID: {document.get('_id', 'Unknown')}")
            raise Exception(error_msg)
    
    def _parse_publish_time(self, document: dict) -> datetime:
        """è§£æå‘å¸ƒæ—¶é—´å­—æ®µ - ä¼˜å…ˆä½¿ç”¨publish_timeï¼Œåå¤‡publish_date"""
        # ğŸ”¥ ä¼˜å…ˆä½¿ç”¨publish_timeå­—æ®µï¼ˆdatetimeå¯¹è±¡ï¼‰
        if document.get("publish_time"):
            parsed_time = self._parse_datetime(document["publish_time"])
            if parsed_time:
                return parsed_time
        
        # ğŸ”¥ åå¤‡ï¼šä½¿ç”¨publish_dateå­—æ®µï¼ˆå­—ç¬¦ä¸²æ ¼å¼YYYY-MM-DDï¼‰
        if document.get("publish_date"):
            try:
                publish_date_str = document["publish_date"]
                if isinstance(publish_date_str, str) and len(publish_date_str) == 10:
                    # YYYY-MM-DDæ ¼å¼ï¼Œè¡¥å…¨æ—¶åˆ†ç§’ä¸º00:00:00
                    return datetime.strptime(publish_date_str + " 00:00:00", "%Y-%m-%d %H:%M:%S")
                else:
                    return self._parse_datetime(publish_date_str)
            except Exception as e:
                logger.warning(f"Failed to parse publish_date: {document.get('publish_date')} - {str(e)}")
        
        # å°è¯•å…¶ä»–ä¸­æ–‡æ—¶é—´å­—æ®µ
        time_candidates = [
            document.get("å‘å¸ƒæ—¶é—´"),
            document.get("å‘å¸ƒæ—¥æœŸ"),
            document.get("created_at"),
            document.get("å¯¼å…¥æ—¶é—´")
        ]
        
        for time_value in time_candidates:
            if time_value:
                parsed_time = self._parse_datetime(time_value)
                if parsed_time:
                    return parsed_time
        
        # æ‰€æœ‰è§£æéƒ½å¤±è´¥ï¼Œè¿”å›å½“å‰æ—¶é—´
        return datetime.utcnow()
    
    def _parse_datetime(self, time_value) -> datetime:
        """è§£ædatetimeå­—æ®µ"""
        if not time_value:
            return datetime.utcnow()
        
        if isinstance(time_value, datetime):
            return time_value
        
        if isinstance(time_value, str):
            try:
                # å°è¯•æ ‡å‡†æ—¥æœŸæ ¼å¼ YYYY-MM-DD
                return datetime.strptime(time_value, "%Y-%m-%d")
            except ValueError:
                try:
                    # å°è¯•ISOæ ¼å¼è§£æ
                    return datetime.fromisoformat(time_value.replace('Z', '+00:00'))
                except ValueError:
                    try:
                        # å°è¯•å…¶ä»–å¸¸è§æ ¼å¼
                        return datetime.strptime(time_value[:19], "%Y-%m-%d %H:%M:%S")
                    except ValueError:
                        logger.warning(f"æ— æ³•è§£ææ—¶é—´æ ¼å¼: {time_value}")
                        return datetime.utcnow()
        
        return datetime.utcnow()
    
    def _ensure_list(self, value) -> list:
        """ç¡®ä¿å€¼æ˜¯åˆ—è¡¨æ ¼å¼"""
        if value is None:
            return []
        if isinstance(value, list):
            return [str(item).strip() for item in value if item and str(item).strip()]
        if isinstance(value, str):
            return [value.strip()] if value.strip() else []
        try:
            # å°è¯•è½¬æ¢ä¸ºå­—ç¬¦ä¸²å†åŒ…è£…ä¸ºåˆ—è¡¨
            return [str(value).strip()] if str(value).strip() else []
        except:
            return []

    def _map_document_type(self, chinese_type: str) -> str:
        """å°†ä¸­æ–‡æ–‡æ¡£ç±»å‹æ˜ å°„ä¸ºè‹±æ–‡ç±»å‹"""
        type_mapping = {
            "æ”¿ç­–æ³•è§„": "policy",
            "è¡Œä¸šèµ„è®¯": "news", 
            "è°ƒä»·å…¬å‘Š": "price",
            "äº¤æ˜“å…¬å‘Š": "announcement"
        }
        return type_mapping.get(chinese_type, "news")

    async def get_content_by_id(self, content_id: str) -> Optional[Content]:
        """æ ¹æ®IDè·å–å†…å®¹"""
        try:
            document = await self.collection.find_one({"_id": ObjectId(content_id)})
            if document:
                return self._map_document_to_content(document)
            return None
        except Exception as e:
            raise Exception(f"Failed to get content by id: {str(e)}")

    async def get_content_list(
        self,
        content_type: str = None,
        tags: List[str] = None,
        skip: int = 0,
        limit: int = 20,
        sort_by: str = "latest"
    ) -> List[Content]:
        """è·å–å†…å®¹åˆ—è¡¨"""
        try:
            query = {}
            
            if content_type:
                query["type"] = content_type
            
            if tags:
                tag_queries = []
                for tag in tags:
                    tag_queries.extend([
                        {"basic_info_tags": tag},
                        {"region_tags": tag},
                        {"energy_type_tags": tag},
                        {"business_field_tags": tag},
                        {"beneficiary_tags": tag},
                        {"policy_measure_tags": tag},
                        {"importance_tags": tag}
                    ])
                query["$or"] = tag_queries
            
            # ğŸ”¥ ä¿®æ”¹æ’åºå­—æ®µï¼šä½¿ç”¨publish_dateæ›¿ä»£publish_time
            if sort_by == "latest":
                sort_field = "publish_date"
                sort_order = -1  # ä»æ–°åˆ°æ—§
            elif sort_by == "oldest":
                sort_field = "publish_date"
                sort_order = 1   # ä»æ—§åˆ°æ–°
            elif sort_by == "popularity":
                sort_field = "view_count"
                sort_order = -1
            else:
                sort_field = "publish_date"
                sort_order = -1
            
            contents = []
            cursor = self.collection.find(query).sort([(sort_field, sort_order)]).skip(skip).limit(limit)
            
            async for document in cursor:
                try:
                    content = self._map_document_to_content(document)
                    contents.append(content)
                except Exception as e:
                    logger.warning(f"è·³è¿‡æ— æ•ˆæ–‡æ¡£ {document.get('_id')}: {str(e)}")
                    continue
            
            return contents
        except Exception as e:
            raise Exception(f"Failed to get content list: {str(e)}")

    async def get_content_count(
        self,
        content_type: str = None,
        tags: List[str] = None,
        search_keyword: str = None
    ) -> int:
        """è·å–å†…å®¹æ€»æ•°"""
        try:
            query = {}
            
            if content_type:
                query["type"] = content_type
            
            if search_keyword:
                query["$or"] = [
                    {"title": {"$regex": search_keyword, "$options": "i"}},
                    {"content": {"$regex": search_keyword, "$options": "i"}}
                ]
            
            if tags:
                tag_queries = []
                for tag in tags:
                    tag_queries.extend([
                        {"basic_info_tags": tag},
                        {"region_tags": tag},
                        {"energy_type_tags": tag},
                        {"business_field_tags": tag},
                        {"beneficiary_tags": tag},
                        {"policy_measure_tags": tag},
                        {"importance_tags": tag}
                    ])
                if "$or" in query:
                    # å¦‚æœå·²ç»æœ‰æœç´¢æ¡ä»¶ï¼Œéœ€è¦ç»“åˆ
                    query = {"$and": [query, {"$or": tag_queries}]}
                else:
                    query["$or"] = tag_queries
            
            count = await self.collection.count_documents(query)
            return count
        except Exception as e:
            raise Exception(f"Failed to get content count: {str(e)}")

    async def get_search_count(
        self,
        keyword: str,
        content_type: str = None,
        tags: List[str] = None
    ) -> int:
        """è·å–æœç´¢ç»“æœæ€»æ•°"""
        return await self.get_content_count(
            content_type=content_type,
            tags=tags,
            search_keyword=keyword
        )

    async def get_all_tags(self) -> List[str]:
        """è·å–æ‰€æœ‰å¯ç”¨çš„æ ‡ç­¾"""
        try:
            pipeline = [
                {
                    "$project": {
                        "all_tags": {
                            "$concatArrays": [
                                "$basic_info_tags",
                                "$region_tags", 
                                "$energy_type_tags",
                                "$business_field_tags",
                                "$beneficiary_tags",
                                "$policy_measure_tags",
                                "$importance_tags"
                            ]
                        }
                    }
                },
                {"$unwind": "$all_tags"},
                {"$group": {"_id": "$all_tags"}},
                {"$sort": {"_id": 1}}
            ]
            
            tags = []
            async for doc in self.collection.aggregate(pipeline):
                if doc['_id']:  # è¿‡æ»¤ç©ºæ ‡ç­¾
                    tags.append(doc['_id'])
            
            return tags
        except Exception as e:
            raise Exception(f"Failed to get all tags: {str(e)}")

    async def increment_view_count(self, content_id: str) -> None:
        """å¢åŠ å†…å®¹æµè§ˆæ¬¡æ•°"""
        try:
            await self.collection.update_one(
                {"_id": ObjectId(content_id)},
                {"$inc": {"view_count": 1}}
            )
        except Exception as e:
            raise Exception(f"Failed to increment view count: {str(e)}")

    async def search_content(
        self,
        keyword: str,
        content_type: str = None,
        tags: List[str] = None,
        skip: int = 0,
        limit: int = 20
    ) -> List[Content]:
        """æœç´¢å†…å®¹"""
        try:
            query = {
                "$or": [
                    {"æ ‡é¢˜": {"$regex": keyword, "$options": "i"}},
                    {"æ–‡ç« å†…å®¹": {"$regex": keyword, "$options": "i"}}
                ]
            }
            
            # æ·»åŠ å†…å®¹ç±»å‹ç­›é€‰
            if content_type:
                reverse_type_mapping = {
                    "policy": "æ”¿ç­–æ³•è§„",
                    "news": "è¡Œä¸šèµ„è®¯",
                    "price": "è°ƒä»·å…¬å‘Š", 
                    "announcement": "äº¤æ˜“å…¬å‘Š"
                }
                chinese_type = reverse_type_mapping.get(content_type, content_type)
                query["basic_info_tags"] = chinese_type
            
            # æ·»åŠ æ ‡ç­¾ç­›é€‰
            if tags:
                tag_queries = []
                for tag in tags:
                    tag_queries.extend([
                        {"basic_info_tags": tag},
                        {"region_tags": tag},
                        {"energy_type_tags": tag},
                        {"business_field_tags": tag},
                        {"beneficiary_tags": tag},
                        {"policy_measure_tags": tag},
                        {"importance_tags": tag}
                    ])
                query = {"$and": [query, {"$or": tag_queries}]}
            
            contents = []
            # ğŸ”¥ ä¿®æ”¹æ’åºå­—æ®µï¼šä½¿ç”¨publish_dateæ›¿ä»£å‘å¸ƒæ—¶é—´
            cursor = self.collection.find(query).sort([("publish_date", -1)]).skip(skip).limit(limit)
            
            async for document in cursor:
                try:
                    content = self._map_document_to_content(document)
                    contents.append(content)
                except Exception as e:
                    logger.warning(f"è·³è¿‡æ— æ•ˆæ–‡æ¡£ {document.get('_id')}: {str(e)}")
                    continue
            
            return contents
        except Exception as e:
            raise Exception(f"Failed to search content: {str(e)}")

    async def get_content_by_user_tags(
        self,
        user_tags: List[str],
        skip: int = 0,
        limit: int = 20
    ) -> List[Content]:
        """æ ¹æ®ç”¨æˆ·æ ‡ç­¾è·å–æ¨èå†…å®¹"""
        try:
            # æ„å»ºå¤æ‚çš„æ ‡ç­¾åŒ¹é…æŸ¥è¯¢
            tag_conditions = []
            tag_fields = [
                'basic_info_tags', 'region_tags', 'energy_type_tags',
                'business_field_tags', 'beneficiary_tags', 
                'policy_measure_tags', 'importance_tags'
            ]
            
            for field in tag_fields:
                tag_conditions.append({field: {"$in": user_tags}})
            
            query = {"$or": tag_conditions}
            
            # ä½¿ç”¨èšåˆç®¡é“è®¡ç®—åŒ¹é…åˆ†æ•°
            pipeline = [
                {"$match": query},
                {
                    "$addFields": {
                        "match_score": {
                            "$sum": [
                                {"$size": {"$setIntersection": ["$basic_info_tags", user_tags]}},
                                {"$size": {"$setIntersection": ["$region_tags", user_tags]}},
                                {"$size": {"$setIntersection": ["$energy_type_tags", user_tags]}},
                                {"$size": {"$setIntersection": ["$business_field_tags", user_tags]}},
                                {"$size": {"$setIntersection": ["$beneficiary_tags", user_tags]}},
                                {"$size": {"$setIntersection": ["$policy_measure_tags", user_tags]}},
                                {"$size": {"$setIntersection": ["$importance_tags", user_tags]}}
                            ]
                        }
                    }
                },
                # ğŸ”¥ ä¿®æ”¹æ’åºå­—æ®µï¼šå…ˆæŒ‰åŒ¹é…åˆ†æ•°ï¼Œå†æŒ‰publish_dateæ’åº
                {"$sort": {"match_score": -1, "publish_date": -1}},
                {"$skip": skip},
                {"$limit": limit}
            ]
            
            contents = []
            async for document in self.collection.aggregate(pipeline):
                try:
                    content = self._map_document_to_content(document)
                    contents.append(content)
                except Exception as e:
                    logger.warning(f"è·³è¿‡æ— æ•ˆæ–‡æ¡£ {document.get('_id')}: {str(e)}")
                    continue
            
            return contents
        except Exception as e:
            raise Exception(f"Failed to get content by user tags: {str(e)}")

    async def get_content_by_tags(
        self,
        basic_info_tags: List[str] = None,
        region_tags: List[str] = None,
        energy_type_tags: List[str] = None,
        business_field_tags: List[str] = None,
        beneficiary_tags: List[str] = None,
        policy_measure_tags: List[str] = None,
        importance_tags: List[str] = None,
        limit: int = 10
    ) -> List[Content]:
        """æ ¹æ®åˆ†ç±»æ ‡ç­¾è·å–å†…å®¹"""
        try:
            # æ„å»ºæŸ¥è¯¢æ¡ä»¶ - ä½¿ç”¨$andç¡®ä¿æ‰€æœ‰æŒ‡å®šçš„æ ‡ç­¾éƒ½åŒ¹é…
            tag_conditions = []
            
            if basic_info_tags:
                tag_conditions.append({"basic_info_tags": {"$in": basic_info_tags}})
            if region_tags:
                tag_conditions.append({"region_tags": {"$in": region_tags}})
            if energy_type_tags:
                tag_conditions.append({"energy_type_tags": {"$in": energy_type_tags}})
            if business_field_tags:
                tag_conditions.append({"business_field_tags": {"$in": business_field_tags}})
            if beneficiary_tags:
                tag_conditions.append({"beneficiary_tags": {"$in": beneficiary_tags}})
            if policy_measure_tags:
                tag_conditions.append({"policy_measure_tags": {"$in": policy_measure_tags}})
            if importance_tags:
                tag_conditions.append({"importance_tags": {"$in": importance_tags}})
            
            if not tag_conditions:
                # å¦‚æœæ²¡æœ‰æ ‡ç­¾æ¡ä»¶ï¼Œè¿”å›æœ€æ–°å†…å®¹
                return await self.get_content_list(limit=limit, sort_by="latest")
            
            # ä½¿ç”¨$andæŸ¥è¯¢ç¡®ä¿æ‰€æœ‰æŒ‡å®šæ ‡ç­¾éƒ½åŒ¹é…
            if len(tag_conditions) == 1:
                query = tag_conditions[0]
            else:
                query = {"$and": tag_conditions}
            
            contents = []
            cursor = self.collection.find(query).sort([("å‘å¸ƒæ—¶é—´", -1)]).limit(limit)
            
            async for document in cursor:
                try:
                    content = self._map_document_to_content(document)
                    contents.append(content)
                except Exception as e:
                    logger.warning(f"è·³è¿‡æ— æ•ˆæ–‡æ¡£ {document.get('_id')}: {str(e)}")
                    continue
            
            return contents
        except Exception as e:
            raise Exception(f"Failed to get content by tags: {str(e)}") 