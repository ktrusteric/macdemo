#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import json
import ast
from typing import List, Dict, Any

def clean_duplicate_fields():
    """
    ç§»é™¤é‡å¤çš„æ–‡æ¡£ç±»å‹å­—æ®µï¼Œç»Ÿä¸€ä½¿ç”¨åŸºç¡€ä¿¡æ¯æ ‡ç­¾
    1. åˆ é™¤"æ–‡æ¡£ç±»å‹"å­—æ®µ
    2. ç¡®ä¿"åŸºç¡€ä¿¡æ¯æ ‡ç­¾"æ ¼å¼æ­£ç¡®
    3. éªŒè¯æ•°æ®ä¸€è‡´æ€§
    """
    
    # è¯»å–å®Œæ•´æ•°æ®é›†
    with open('ä¸Šæµ·çŸ³æ²¹å¤©ç„¶æ°”äº¤æ˜“ä¸­å¿ƒä¿¡æ¯é—¨æˆ·ç³»ç»Ÿ_å®Œæ•´æ•°æ®é›†_51ç¯‡.json', 'r', encoding='utf-8') as f:
        articles = json.load(f)
    
    print('ğŸ”§ æ¸…ç†é‡å¤å­—æ®µï¼šæ–‡æ¡£ç±»å‹ vs åŸºç¡€ä¿¡æ¯æ ‡ç­¾')
    print('=' * 60)
    
    # ç»Ÿè®¡ä¿¡æ¯
    duplicate_count = 0
    cleaned_articles = []
    issues = []
    
    for i, article in enumerate(articles):
        # è·å–åŸå§‹å­—æ®µ
        doc_type = article.get('æ–‡æ¡£ç±»å‹', '')
        basic_info_str = article.get('åŸºç¡€ä¿¡æ¯æ ‡ç­¾', '')
        
        # è§£æåŸºç¡€ä¿¡æ¯æ ‡ç­¾
        basic_info_tags = []
        if isinstance(basic_info_str, str) and basic_info_str.strip():
            try:
                if basic_info_str.startswith('['):
                    basic_info_tags = ast.literal_eval(basic_info_str)
                else:
                    basic_info_tags = [basic_info_str]
            except:
                basic_info_tags = [basic_info_str]
        elif isinstance(basic_info_str, list):
            basic_info_tags = basic_info_str
        
        # æ£€æŸ¥é‡å¤æ€§
        is_duplicate = (len(basic_info_tags) == 1 and basic_info_tags[0] == doc_type)
        if is_duplicate:
            duplicate_count += 1
        
        # æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥
        if doc_type and not basic_info_tags:
            # å¦‚æœæœ‰æ–‡æ¡£ç±»å‹ä½†æ²¡æœ‰åŸºç¡€ä¿¡æ¯æ ‡ç­¾ï¼Œä½¿ç”¨æ–‡æ¡£ç±»å‹å¡«å……
            basic_info_tags = [doc_type]
            issues.append(f'æ–‡ç« {i+1}: è¡¥å……ç¼ºå¤±çš„åŸºç¡€ä¿¡æ¯æ ‡ç­¾: {doc_type}')
        elif not doc_type and basic_info_tags:
            # æ­£å¸¸æƒ…å†µï¼Œæœ‰åŸºç¡€ä¿¡æ¯æ ‡ç­¾
            pass
        elif doc_type and basic_info_tags and not is_duplicate:
            # ä¸ä¸€è‡´çš„æƒ…å†µï¼Œä¼˜å…ˆä¿ç•™åŸºç¡€ä¿¡æ¯æ ‡ç­¾
            issues.append(f'æ–‡ç« {i+1}: å­—æ®µä¸ä¸€è‡´ï¼Œä¿ç•™åŸºç¡€ä¿¡æ¯æ ‡ç­¾: {basic_info_tags}')
        
        # æ¸…ç†æ–‡ç« æ•°æ®
        cleaned_article = {k: v for k, v in article.items() if k != 'æ–‡æ¡£ç±»å‹'}
        
        # ç¡®ä¿åŸºç¡€ä¿¡æ¯æ ‡ç­¾ä¸ºæ•°ç»„æ ¼å¼
        if basic_info_tags:
            cleaned_article['basic_info_tags'] = basic_info_tags
        
        # åˆ é™¤æ—§çš„ä¸­æ–‡å­—æ®µåï¼Œä½¿ç”¨è‹±æ–‡æ ‡å‡†å­—æ®µ
        if 'åŸºç¡€ä¿¡æ¯æ ‡ç­¾' in cleaned_article:
            if 'basic_info_tags' not in cleaned_article:
                cleaned_article['basic_info_tags'] = basic_info_tags
            del cleaned_article['åŸºç¡€ä¿¡æ¯æ ‡ç­¾']
        
        cleaned_articles.append(cleaned_article)
        
        if i < 5:  # æ˜¾ç¤ºå‰5ç¯‡çš„å¤„ç†æƒ…å†µ
            print(f'æ–‡ç« {i+1:2d}: åŸæ–‡æ¡£ç±»å‹="{doc_type}" | åŸºç¡€ä¿¡æ¯æ ‡ç­¾={basic_info_tags} | {"âœ…åˆ é™¤é‡å¤" if is_duplicate else "âš ï¸ä¸ä¸€è‡´"}')
    
    print(f'\nğŸ“Š æ¸…ç†ç»Ÿè®¡:')
    print(f'æ€»æ–‡ç« æ•°: {len(articles)}')
    print(f'é‡å¤å­—æ®µæ•°: {duplicate_count}')
    print(f'é‡å¤æ¯”ä¾‹: {duplicate_count/len(articles)*100:.1f}%')
    print(f'å‘ç°é—®é¢˜: {len(issues)}')
    
    if issues:
        print(f'\nâš ï¸ æ•°æ®é—®é¢˜æ¸…å•:')
        for issue in issues[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
            print(f'  {issue}')
        if len(issues) > 10:
            print(f'  ... è¿˜æœ‰ {len(issues)-10} ä¸ªé—®é¢˜')
    
    # éªŒè¯æ¸…ç†åçš„æ•°æ®
    basic_info_distribution = {}
    for article in cleaned_articles:
        basic_tags = article.get('basic_info_tags', [])
        for tag in basic_tags:
            basic_info_distribution[tag] = basic_info_distribution.get(tag, 0) + 1
    
    print(f'\nğŸ“‹ æ¸…ç†ååŸºç¡€ä¿¡æ¯æ ‡ç­¾åˆ†å¸ƒ:')
    for tag, count in sorted(basic_info_distribution.items()):
        print(f'  {tag}: {count}ç¯‡')
    
    # ä¿å­˜æ¸…ç†åçš„æ•°æ®
    output_file = 'ä¸Šæµ·çŸ³æ²¹å¤©ç„¶æ°”äº¤æ˜“ä¸­å¿ƒä¿¡æ¯é—¨æˆ·ç³»ç»Ÿ_æ¸…ç†é‡å¤å­—æ®µ_51ç¯‡.json'
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(cleaned_articles, f, ensure_ascii=False, indent=2)
    
    print(f'\nâœ… æ¸…ç†å®Œæˆï¼')
    print(f'ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_file}')
    print(f'ğŸ—‘ï¸  å·²åˆ é™¤å­—æ®µ: "æ–‡æ¡£ç±»å‹"')
    print(f'ğŸ“ å·²æ ‡å‡†åŒ–å­—æ®µ: "åŸºç¡€ä¿¡æ¯æ ‡ç­¾" â†’ "basic_info_tags"')
    
    return cleaned_articles, basic_info_distribution

def validate_cleaned_data():
    """éªŒè¯æ¸…ç†åçš„æ•°æ®è´¨é‡"""
    try:
        with open('ä¸Šæµ·çŸ³æ²¹å¤©ç„¶æ°”äº¤æ˜“ä¸­å¿ƒä¿¡æ¯é—¨æˆ·ç³»ç»Ÿ_æ¸…ç†é‡å¤å­—æ®µ_51ç¯‡.json', 'r', encoding='utf-8') as f:
            cleaned_data = json.load(f)
        
        print(f'\nğŸ” æ•°æ®éªŒè¯:')
        print(f'æ–‡ç« æ€»æ•°: {len(cleaned_data)}')
        
        # æ£€æŸ¥å­—æ®µå­˜åœ¨æ€§
        has_doc_type = sum(1 for article in cleaned_data if 'æ–‡æ¡£ç±»å‹' in article)
        has_basic_info = sum(1 for article in cleaned_data if 'basic_info_tags' in article)
        
        print(f'åŒ…å«"æ–‡æ¡£ç±»å‹"å­—æ®µçš„æ–‡ç« : {has_doc_type} (åº”ä¸º0)')
        print(f'åŒ…å«"basic_info_tags"å­—æ®µçš„æ–‡ç« : {has_basic_info}')
        
        if has_doc_type == 0:
            print('âœ… "æ–‡æ¡£ç±»å‹"å­—æ®µå·²å®Œå…¨ç§»é™¤')
        else:
            print('âŒ ä»æœ‰"æ–‡æ¡£ç±»å‹"å­—æ®µæ®‹ç•™')
        
        return cleaned_data
        
    except FileNotFoundError:
        print('âŒ æ¸…ç†åçš„æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œæ¸…ç†æµç¨‹')
        return None

if __name__ == '__main__':
    # æ‰§è¡Œæ¸…ç†
    cleaned_articles, distribution = clean_duplicate_fields()
    
    # éªŒè¯ç»“æœ
    validate_cleaned_data()
    
    print(f'\nğŸ¯ æ€»ç»“:')
    print(f'âœ… æˆåŠŸç§»é™¤é‡å¤çš„"æ–‡æ¡£ç±»å‹"å­—æ®µ')
    print(f'âœ… ç»Ÿä¸€ä½¿ç”¨"basic_info_tags"æ ‡å‡†å­—æ®µ')
    print(f'âœ… æ•°æ®æ ¼å¼æ ‡å‡†åŒ–å®Œæˆ')
    print(f'âœ… ä¸ºåç»­å‰åç«¯ç»Ÿä¸€å¥ å®šåŸºç¡€') 